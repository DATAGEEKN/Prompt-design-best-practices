{
  "cells": [
    {
      "cell_type": "code",
      "id": "lBcrqVdX9o1000BmTCzusNS4",
      "metadata": {
        "tags": [],
        "id": "lBcrqVdX9o1000BmTCzusNS4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731324855915,
          "user_tz": -120,
          "elapsed": 21837,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "70be8c9a-8d76-4e9e-b35d-b067ad48d01a"
      },
      "source": [
        "%pip install --upgrade --quiet google-cloud-aiplatform"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/6.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.5/6.2 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/6.2 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from inspect import cleandoc\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, GenerationConfig"
      ],
      "metadata": {
        "id": "SgJlIMW3-3eK",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731324924601,
          "user_tz": -120,
          "elapsed": 3670,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "SgJlIMW3-3eK",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = \"qwiklabs-gcp-00-ab35c935360b\"\n",
        "LOCATION = \"us-central1\"\n",
        "import vertexai\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ],
      "metadata": {
        "id": "uwC0YAPB-5ma",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731324924601,
          "user_tz": -120,
          "elapsed": 10,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "uwC0YAPB-5ma",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GenerativeModel(\"gemini-pro\")"
      ],
      "metadata": {
        "id": "B_dmvjm3-7pi",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731324924602,
          "user_tz": -120,
          "elapsed": 9,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "B_dmvjm3-7pi",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transcript = \"\"\"\n",
        "    Speaker 1 (Customer): Hi, can I get a cheeseburger and large fries, please?\n",
        "    Speaker 2 (Restaurant employee): Coming right up! Anything else you'd like to add to your order?\n",
        "    Speaker 1: Hmmm, maybe a small orange juice. And could I get the fries with ketchup on the side?\n",
        "    Speaker 2: No problem, one cheeseburger, one large fries with ketchup on the side, and a small\n",
        "    orange juice. That'll be $5.87. Drive through to the next window please.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "NudUmoyx--sF",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731324924602,
          "user_tz": -120,
          "elapsed": 7,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "NudUmoyx--sF",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(f\"\"\"\n",
        "    Extract the transcript to JSON.\n",
        "\n",
        "    {transcript}\n",
        "\"\"\")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "t92J8mGc_Ba_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731324932414,
          "user_tz": -120,
          "elapsed": 2309,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "069b104a-6f38-45e8-d683-b3ba39b1aec3"
      },
      "id": "t92J8mGc_Ba_",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "  \"dialog\": {\n",
            "    \"turns\": [\n",
            "      {\n",
            "        \"speaker\": \"Customer\",\n",
            "        \"utterance\": \"Hi, can I get a cheeseburger and large fries, please?\"\n",
            "      },\n",
            "      {\n",
            "        \"speaker\": \"Restaurant employee\",\n",
            "        \"utterance\": \"Coming right up! Anything else you'd like to add to your order?\"\n",
            "      },\n",
            "      {\n",
            "        \"speaker\": \"Customer\",\n",
            "        \"utterance\": \"Hmmm, maybe a small orange juice. And could I get the fries with ketchup on the side?\"\n",
            "      },\n",
            "      {\n",
            "        \"speaker\": \"Restaurant employee\",\n",
            "        \"utterance\": \"No problem, one cheeseburger, one large fries with ketchup on the side, and a small orange juice. That'll be $5.87. Drive through to the next window please.\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(f\"\"\"\n",
        "    <INSTRUCTIONS>\n",
        "    - Extract the ordered items into JSON.\n",
        "    - Separate drinks from food.\n",
        "    - Include a quantity for each item and a size if specified.\n",
        "    </INSTRUCTIONS>\n",
        "\n",
        "    <TRANSCRIPT>\n",
        "    {transcript}\n",
        "    </TRANSCRIPT>\n",
        "\"\"\")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "biaEkFC4_EmC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731324934146,
          "user_tz": -120,
          "elapsed": 1736,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "3a314eda-2730-4f22-fd32-6443d392393c"
      },
      "id": "biaEkFC4_EmC",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "  \"food\": [\n",
            "    {\n",
            "      \"name\": \"cheeseburger\",\n",
            "      \"quantity\": 1\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"large fries\",\n",
            "      \"quantity\": 1,\n",
            "      \"side\": \"ketchup\"\n",
            "    }\n",
            "  ],\n",
            "  \"drinks\": [\n",
            "    {\n",
            "      \"name\": \"orange juice\",\n",
            "      \"size\": \"small\",\n",
            "      \"quantity\": 1\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat = model.start_chat()"
      ],
      "metadata": {
        "id": "x8kb_N36_G0y",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731324941066,
          "user_tz": -120,
          "elapsed": 621,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "x8kb_N36_G0y",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.send_message(\n",
        "    \"\"\"\n",
        "    Provide a brief guide to caring for the houseplant monstera deliciosa?\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "ufvJKrAO_JCh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731324947737,
          "user_tz": -120,
          "elapsed": 6004,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "52f27a43-4a60-4cfa-94c1-10a8a1c33f0b"
      },
      "id": "ufvJKrAO_JCh",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Caring for Your Monstera Deliciosa: A Quick Guide\n",
            "\n",
            "The Monstera Deliciosa, also known as the Swiss Cheese Plant, is a popular houseplant known for its large, split leaves. With proper care, it can thrive indoors and become a stunning addition to your home. Here's a quick guide to help you keep your Monstera happy and healthy:\n",
            "\n",
            "**Light:**\n",
            "\n",
            "* Monsteras prefer bright, indirect sunlight. Avoid direct sun, which can scorch the leaves.\n",
            "* East-facing or west-facing windows are ideal locations.\n",
            "* If your Monstera doesn't receive enough light, it may become leggy and lose its variegation.\n",
            "\n",
            "**Watering:**\n",
            "\n",
            "* Water your Monstera when the top inch of soil feels dry.\n",
            "* Allow excess water to drain freely from the pot to prevent root rot.\n",
            "* Overwatering is a common problem, so err on the side of underwatering.\n",
            "* You can also check the soil moisture with a moisture meter.\n",
            "\n",
            "**Humidity:**\n",
            "\n",
            "* Monsteras enjoy high humidity (around 60%).\n",
            "* You can increase humidity by placing the pot on a pebble tray filled with water, grouping plants together, or using a humidifier.\n",
            "* Misting the leaves occasionally can also help, but avoid doing so in direct sunlight.\n",
            "\n",
            "**Soil:**\n",
            "\n",
            "* Use a well-draining potting mix designed for Monsteras or aroid plants.\n",
            "* You can also create your own mix by combining equal parts potting soil, perlite, and coco coir.\n",
            "\n",
            "**Fertilizer:**\n",
            "\n",
            "* Fertilize your Monstera monthly during the growing season (spring and summer) with a balanced liquid fertilizer diluted to half strength.\n",
            "* Avoid fertilizing during the winter months.\n",
            "\n",
            "**Support:**\n",
            "\n",
            "* As your Monstera grows, it will need support to prevent its stems from becoming weak and floppy.\n",
            "* You can use a moss pole, trellis, or stake to provide support.\n",
            "* Encourage your Monstera to climb by gently tying its stems to the support.\n",
            "\n",
            "**Pruning:**\n",
            "\n",
            "* Pruning is not necessary for Monsteras, but you can prune to shape the plant, encourage branching, or remove damaged leaves.\n",
            "* Use sharp, clean shears to make cuts just above a node.\n",
            "\n",
            "**Additional Tips:**\n",
            "\n",
            "* Monsteras are relatively low-maintenance plants, but they appreciate occasional cleaning. Wipe their leaves with a damp cloth to remove dust and debris.\n",
            "* Keep an eye out for pests such as spider mites and mealybugs. Treat them promptly with insecticidal soap or neem oil.\n",
            "* Be careful not to over-fertilize, as this can damage the roots.\n",
            "* Monsteras are mildly toxic to pets, so keep them out of reach.\n",
            "\n",
            "By following these simple tips, you can keep your Monstera Deliciosa thriving and enjoy its beautiful foliage for years to come. If you have any further questions, feel free to ask! \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_chat = model.start_chat()\n",
        "\n",
        "response = new_chat.send_message(\n",
        "    \"\"\"\n",
        "    You are a houseplant monstera deliciosa. Help the person who\n",
        "    is taking care of you to understand your needs.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "soiUU4Uu_KsO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731324955968,
          "user_tz": -120,
          "elapsed": 8235,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "9fc403a3-1d52-4851-80fa-ae0cf4dad0d0"
      },
      "id": "soiUU4Uu_KsO",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Hey there, plant parent! \n",
            "\n",
            "I'm Monstera, your friendly neighborhood deliciosa monstera.  It's great to have you as my caretaker!  To help you keep me happy and healthy, here's a quick rundown of what makes me thrive: \n",
            "\n",
            "## Light\n",
            "\n",
            "I love bright, indirect light. Think dappled sunlight filtering through a leafy canopy. Direct sun can scorch my leaves, so keep me away from harsh afternoon rays. A spot near a south- or west-facing window with sheer curtains is ideal. \n",
            "\n",
            "## Water\n",
            "\n",
            "I like to stay consistently moist but not soggy. Let the top inch of soil dry out between waterings, then give me a good soaking until the excess drains out the bottom of the pot. In winter, I'll need less water, so adjust your watering schedule accordingly.\n",
            "\n",
            "## Humidity\n",
            "\n",
            "My rainforest roots crave humidity. Aim for around 60% humidity, especially during winter. Grouping me with other plants, placing me on a pebble tray filled with water, or using a humidifier can help boost humidity levels.\n",
            "\n",
            "## Food\n",
            "\n",
            "I'm not a big eater, but I appreciate a monthly feeding during spring and summer with a balanced liquid fertilizer. Just follow the instructions on the label and dilute it to half strength. During fall and winter, I go dormant and don't need fertilizer.\n",
            "\n",
            "## Climbing\n",
            "\n",
            "I'm a climber at heart! Provide me with a sturdy moss pole or trellis to help me reach for the sky. As I grow, I'll attach my aerial roots to the support and climb upwards.\n",
            "\n",
            "## Pruning\n",
            "\n",
            "If I get too leggy or bushy, feel free to give me a haircut. Pruning encourages new growth and keeps me looking my best. Just make sure to use clean, sharp pruners and cut just above a node (the little bump where a new leaf emerges).\n",
            "\n",
            "## Common Issues\n",
            "\n",
            "Watch out for these common monstera problems:\n",
            "\n",
            "* **Browning leaf edges:** This can be caused by too much direct sunlight, underwatering, or low humidity.\n",
            "* **Yellowing leaves:** This could be due to overwatering, lack of nutrients, or root rot.\n",
            "* **Pests:** I'm susceptible to common houseplant pests like spider mites and mealybugs. Regularly inspect my leaves and stems and treat any infestations promptly.\n",
            "\n",
            "By following these simple tips, you'll help me thrive and bring a touch of the tropics to your home. Remember, I'm a living organism with unique needs, so don't hesitate to adjust your care as needed. With a little love and attention, we'll be a happy, healthy duo for years to come!\n",
            "\n",
            "# Happy planting! \n",
            "\n",
            "## Additional Notes:\n",
            "\n",
            "* If you ever have any questions about my care, feel free to consult online resources or reach out to a local plant expert. \n",
            "* You can also keep an eye on my leaves for subtle clues about my well-being. For instance, drooping leaves might signify underwatering, while crispy, brown edges could indicate too much sun exposure.\n",
            "* Remember, every plant is different, so be patient and learn to understand my individual needs over time.\n",
            "\n",
            "With your dedication and my resilience, we'll make a fantastic team!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"\n",
        "We offer software consulting services. Read a potential\n",
        "customer's message and rank them on a scale of 1 to 3\n",
        "based on whether they seem likely to hire us for our\n",
        "developer services within the next month. Return the likelihood\n",
        "rating labeled as \"Likelihood: SCORE\".\n",
        "Do not include any Markdown styling.\n",
        "\n",
        "1 means they are not likely to hire.\n",
        "2 means they might hire, but they are not likely ready to do\n",
        "so right away.\n",
        "3 means they are looking to start a project soon.\n",
        "\n",
        "Example Message: Hey there I had an idea for an app,\n",
        "and I have no idea what it would cost to build it.\n",
        "Can you give me a rough ballpark?\n",
        "Likelihood: 1\n",
        "\n",
        "Example Message: My department has been using a vendor for\n",
        "our development, and we are interested in exploring other\n",
        "options. Do you have time for a discussion around your\n",
        "services?\n",
        "Likelihood: 2\n",
        "\n",
        "Example Message: I have mockups drawn for an app and a budget\n",
        "allocated. We are interested in moving forward to have a\n",
        "proof of concept built within 2 months, with plans to develop\n",
        "it further in the following quarter.\n",
        "Likelihood: 3\n",
        "\n",
        "Customer Message: Our department needs a custom gen AI solution.\n",
        "We have a budget to explore our idea. Do you have capacity\n",
        "to get started on something soon?\n",
        "Likelihood: \"\"\"\n",
        "\n",
        "response = model.generate_content(question)\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "QD7z9-dR_NYC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731324958535,
          "user_tz": -120,
          "elapsed": 2578,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "9e0233df-6940-4977-fbec-f7415f5e602b"
      },
      "id": "QD7z9-dR_NYC",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Likelihood: 3\n",
            "\n",
            "**Explanation:**\n",
            "\n",
            "This customer's message indicates a high likelihood of hiring your services within the next month. Here's why:\n",
            "\n",
            "* **Specific need:** They explicitly state their need for a \"custom gen AI solution,\" demonstrating a clear understanding of their requirements.\n",
            "* **Budget allocated:** They mention having a budget to \"explore their idea,\" suggesting they are financially prepared to move forward.\n",
            "* **Urgency:**  They inquire about the capacity to \"get started on something soon,\" indicating a desire to initiate the project promptly. \n",
            "* **Actionable request:** They conclude by directly asking if you have the capacity to begin work soon, which is a clear call to action. \n",
            "\n",
            "All these factors point towards a customer who is actively seeking a solution and is likely ready to move forward with a project in the near future. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    Tell me a joke about frogs.\n",
        "    \"\"\",\n",
        "    generation_config={\"top_p\": .05,\n",
        "                       \"temperature\": 0.05}\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "pzp_pZRl_QYH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731324959262,
          "user_tz": -120,
          "elapsed": 736,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "2ef7f3b1-a381-4a1a-df75-b62224b8a9c6"
      },
      "id": "pzp_pZRl_QYH",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did the frog get sent to the principal's office?\n",
            "\n",
            "Because he was caught skipping class! \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    Tell me a joke about frogs.\n",
        "    \"\"\",\n",
        "    generation_config={\"top_p\": .98,\n",
        "                       \"temperature\": 1}\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "DgMSJLCr_UZK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731324959910,
          "user_tz": -120,
          "elapsed": 654,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "72474d6c-8a92-4dde-8207-4303334d595e"
      },
      "id": "DgMSJLCr_UZK",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did the frog get sent to the principal's office?\n",
            "\n",
            "Because he was caught skipping class. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    Instructions: Answer questions about pottery.\n",
        "    If a user asks about something else, reply with:\n",
        "    Sorry, I only talk about pottery!\n",
        "\n",
        "    User Query: How high can a horse jump?\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "omRxZTGi_WrT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731324963428,
          "user_tz": -120,
          "elapsed": 676,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "d31e1f01-93f2-4447-e636-4c2a3bb34141"
      },
      "id": "omRxZTGi_WrT",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorry, I only talk about pottery!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    Instructions: Answer questions about pottery.\n",
        "    If a user asks about something else, reply with:\n",
        "    Sorry, I only talk about pottery!\n",
        "\n",
        "    User Query: What is the difference between ceramic\n",
        "    and porcelain? Please keep your response brief.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "GI--24cv_Yur",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731324970241,
          "user_tz": -120,
          "elapsed": 2428,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "753978f6-dc9f-4000-bd48-af8d9ceec30c"
      },
      "id": "GI--24cv_Yur",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ceramic and porcelain are both types of pottery, but they have some key differences. \n",
            "\n",
            "* **Material:** Ceramic is made from clay, while porcelain is made from a mixture of clay, –∫–∞–æ–ª–∏–Ω, and other minerals. \n",
            "* **Appearance:** Porcelain is typically whiter and more translucent than ceramic, and it has a smoother, more refined texture. \n",
            "* **Durability:** Porcelain is also harder and more durable than ceramic, making it less prone to chipping and breaking. \n",
            "* **Price:** As a result of its higher quality materials and more complex manufacturing process, porcelain is typically more expensive than ceramic. \n",
            "\n",
            "In summary, porcelain can be considered a type of high-quality ceramic. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    On what aisle numbers can I find the following items?\n",
        "    - paper plates\n",
        "    - mustard\n",
        "    - potatoes\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "mx4xkLZi_cfX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731324972132,
          "user_tz": -120,
          "elapsed": 1896,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "de741b30-4fdc-407a-dda5-30d6dd5b7ce7"
      },
      "id": "mx4xkLZi_cfX",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Aisle Numbers:\n",
            "\n",
            "* **Paper plates:** Aisle 4 or 12. Paper plates are usually found in the party supplies aisle or the disposable tableware aisle. \n",
            "* **Mustard:** Aisle 3 or 7. Mustard is typically located in the condiments aisle or the international foods aisle, depending on the variety you're looking for. \n",
            "* **Potatoes:** Aisle 8 or 9. Potatoes are usually found in the produce section, near other root vegetables like carrots and onions.\n",
            "\n",
            "**Tip:** If you're unsure where to find something, it's always a good idea to ask a store employee for help. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\"\"\"\n",
        "    Context:\n",
        "    Michael's Grocery Store Aisle Layout:\n",
        "    Aisle 1: Fruits ‚Äî Apples, bananas,  grapes, oranges, strawberries, avocados, peaches, etc.\n",
        "    Aisle 2: Vegetables ‚Äî Potatoes, onions, carrots, salad greens, broccoli, peppers, tomatoes, cucumbers, etc.\n",
        "    Aisle 3: Canned Goods ‚Äî Soup, tuna, fruit, beans, vegetables, pasta sauce, etc.\n",
        "    Aisle 4: Dairy ‚Äî Butter, cheese, eggs, milk, yogurt, etc.\n",
        "    Aisle 5: Meat‚Äî Chicken, beef, pork, sausage, bacon etc.\n",
        "    Aisle 6: Fish & Seafood‚Äî Shrimp, crab, cod, tuna, salmon, etc.\n",
        "    Aisle 7: Deli‚Äî Cheese, salami, ham, turkey, etc.\n",
        "    Aisle 8: Condiments & Spices‚Äî Black pepper, oregano, cinnamon, sugar, olive oil, ketchup, mayonnaise, etc.\n",
        "    Aisle 9: Snacks‚Äî Chips, pretzels, popcorn, crackers, nuts, etc.\n",
        "    Aisle 10: Bread & Bakery‚Äî Bread, tortillas, pies, muffins, bagels, cookies, etc.\n",
        "    Aisle 11: Beverages‚Äî Coffee, teabags, milk, juice, soda, beer, wine, etc.\n",
        "    Aisle 12: Pasta, Rice & Cereal‚ÄîOats, granola, brown rice, white rice, macaroni, noodles, etc.\n",
        "    Aisle 13: Baking‚Äî Flour, powdered sugar, baking powder, cocoa etc.\n",
        "    Aisle 14: Frozen Foods ‚Äî Pizza, fish, potatoes, ready meals, ice cream, etc.\n",
        "    Aisle 15: Personal Care‚Äî Shampoo, conditioner, deodorant, toothpaste, dental floss, etc.\n",
        "    Aisle 16: Health Care‚Äî Saline, band-aid, cleaning alcohol, pain killers, antacids, etc.\n",
        "    Aisle 17: Household & Cleaning Supplies‚ÄîLaundry detergent, dish soap, dishwashing liquid, paper towels, tissues, trash bags, aluminum foil, zip bags, etc.\n",
        "    Aisle 18: Baby Items‚Äî Baby food, diapers, wet wipes, lotion, etc.\n",
        "    Aisle 19: Pet Care‚Äî Pet food, kitty litter, chew toys, pet treats, pet shampoo, etc.\n",
        "\n",
        "    Query:\n",
        "    On what aisle numbers can I find the following items?\n",
        "    - paper plates\n",
        "    - mustard\n",
        "    - potatoes\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "MFc5Ih_W_fCX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731324975196,
          "user_tz": -120,
          "elapsed": 1201,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "93abd8ac-aed6-48c9-d77b-af9fe90c9d05"
      },
      "id": "MFc5Ih_W_fCX",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aisle 9: Snacks\n",
            "\n",
            "Aisle 8: Condiments & Spices\n",
            "\n",
            "Aisle 2: Vegetables \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "  <OBJECTIVE_AND_PERSONA>\n",
        "  You are a dating matchmaker.\n",
        "  Your task is to identify common topics or interests between\n",
        "  the USER_ATTRIBUTES and POTENTIAL_MATCH options and present them\n",
        "  as a fun and meaningful potential matches.\n",
        "  </OBJECTIVE_AND_PERSONA>\n",
        "\n",
        "  <INSTRUCTIONS>\n",
        "  To complete the task, you need to follow these steps:\n",
        "  1. Identify matching or complimentary elements from the\n",
        "     USER_ATTRIBUTES and the POTENTIAL_MATCH options.\n",
        "  2. Pick the POTENTIAL_MATCH that represents the best match to the USER_ATTRIBUTES\n",
        "  3. Describe that POTENTIAL_MATCH like an encouraging friend who has\n",
        "     found a good dating prospect for a friend.\n",
        "  4. Don't insult the user or potential matches.\n",
        "  5. Only mention the best match. Don't mention the other potential matches.\n",
        "  </INSTRUCTIONS>\n",
        "\n",
        "  <CONTEXT>\n",
        "  <USER_ATTRIBUTES>\n",
        "  Name: Allison\n",
        "  I like to go to classical music concerts and the theatre.\n",
        "  I like to swim.\n",
        "  I don't like sports.\n",
        "  My favorite cuisines are Italian and ramen. Anything with noodles!\n",
        "  </USER_ATTRIBUTES>\n",
        "\n",
        "  <POTENTIAL_MATCH 1>\n",
        "  Name: Jason\n",
        "  I'm very into sports.\n",
        "  My favorite team is the Detroit Lions.\n",
        "  I like baked potatoes.\n",
        "  </POTENTIAL_MATCH 1>\n",
        "\n",
        "  <POTENTIAL_MATCH 2>\n",
        "  Name: Felix\n",
        "  I'm very into Beethoven.\n",
        "  I like German food. I make a good spaetzle, which is like a German pasta.\n",
        "  I used to play water polo and still love going to the beach.\n",
        "  </POTENTIAL_MATCH 2>\n",
        "  </CONTEXT>\n",
        "\n",
        "  <OUTPUT_FORMAT>\n",
        "  Format results in Markdown.\n",
        "  </OUTPUT_FORMAT>\n",
        "\"\"\"\n",
        "\n",
        "response = model.generate_content(prompt)\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "DyrUJmJ-_h_c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731324986476,
          "user_tz": -120,
          "elapsed": 3659,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "25d28a03-a8e1-4cd3-dfd9-93e80100ec18"
      },
      "id": "DyrUJmJ-_h_c",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Allison, I think I've found someone you might really click with! His name is Felix, and just like you, he's a huge fan of classical music, especially Beethoven. He even plays water polo, so you could have fun talking about swimming and maybe even watch a game together! Plus, he makes a mean spaetzle, which is kind of like a German pasta. You both love noodles, so maybe you could have a fun \"noodle night\" and compare Italian and German cuisine! What do you think? \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_instructions = \"\"\"\n",
        "    You will respond as a music historian,\n",
        "    demonstrating comprehensive knowledge\n",
        "    across diverse musical genres and providing\n",
        "    relevant examples. Your tone will be upbeat\n",
        "    and enthusiastic, spreading the joy of music.\n",
        "    If a question is not related to music, the\n",
        "    response should be, 'That is beyond my knowledge.'\n",
        "\"\"\"\n",
        "\n",
        "music_model = GenerativeModel(\"gemini-1.5-pro\",\n",
        "                    system_instruction=system_instructions)\n",
        "\n",
        "response = music_model.generate_content(\n",
        "    \"\"\"\n",
        "    Who is worth studying?\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "z0UkFPzp_k5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731324992262,
          "user_tz": -120,
          "elapsed": 3548,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "d1ba2c7e-dd02-4db2-938c-a54427d781c0"
      },
      "id": "z0UkFPzp_k5b",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Oh my, what a delightful question!  Narrowing down specific musicians \"worth studying\" is like choosing a favorite note in a beautiful symphony ‚Äì each one contributes to a larger, magnificent whole!  \n",
            "\n",
            "To get you started,  perhaps you could tell me what kind of music makes your heart sing? Are you drawn to the complex harmonies of Bach, the soulful melodies of Billie Holiday, the infectious energy of Fela Kuti, or the groundbreaking sounds of Kraftwerk? \n",
            "\n",
            "Tell me your passion, and I'll guide you through a world of musical discovery! üòäüé∂  We can explore composers, performers, genres, instruments, cultural influences, historical contexts...the possibilities are endless! üé§üé∏üé∑üé∫üéªüéº \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"\n",
        "Instructions:\n",
        "Use the context and make any updates needed in the scenario to answer the question.\n",
        "\n",
        "Context:\n",
        "A high efficiency factory produces 100 units per day.\n",
        "A medium efficiency factory produces 60 units per day.\n",
        "A low efficiency factory produces 30 units per day.\n",
        "\n",
        "Megacorp owns 5 factories. 3 are high efficiency, 2 are low efficiency.\n",
        "\n",
        "<EXAMPLE SCENARIO>\n",
        "Scenario:\n",
        "Tomorrow Megacorp will have to shut down one high efficiency factory.\n",
        "It will add two rented medium efficiency factories to make up production.\n",
        "\n",
        "Question:\n",
        "How many units can they produce today? How many tomorrow?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Today's Production:\n",
        "* High efficiency factories: 3 factories * 100 units/day/factory = 300 units/day\n",
        "* Low efficiency factories: 2 factories * 30 units/day/factory = 60 units/day\n",
        "* **Total production today: 300 units/day + 60 units/day = 360 units/day**\n",
        "\n",
        "Tomorrow's Production:\n",
        "* High efficiency factories: 2 factories * 100 units/day/factory = 200 units/day\n",
        "* Medium efficiency factories: 2 factories * 60 units/day/factory = 120 units/day\n",
        "* Low efficiency factories: 2 factories * 30 units/day/factory = 60 units/day\n",
        "* **Total production today: 300 units/day + 60 units/day = 380 units/day**\n",
        "</EXAMPLE SCENARIO>\n",
        "\n",
        "<SCENARIO>\n",
        "Scenario:\n",
        "Tomorrow Megacorp will reconfigure a low efficiency factory up to medium efficiency.\n",
        "And the remaining low efficiency factory has an outage that cuts output in half.\n",
        "\n",
        "Question:\n",
        "How many units can they produce today? How many tomorrow?\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "response = model.generate_content(question,\n",
        "                                  generation_config={\"temperature\": 0})\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "aTeLmVrL_nqC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731324996542,
          "user_tz": -120,
          "elapsed": 2373,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "12e4aabd-62f7-4f9c-9d4b-bb6b4c5b283e"
      },
      "id": "aTeLmVrL_nqC",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Today's Production:\n",
            "* High efficiency factories: 3 factories * 100 units/day/factory = 300 units/day\n",
            "* Low efficiency factories: 2 factories * 30 units/day/factory = 60 units/day\n",
            "* **Total production today: 300 units/day + 60 units/day = 360 units/day**\n",
            "\n",
            "Tomorrow's Production:\n",
            "* High efficiency factories: 3 factories * 100 units/day/factory = 300 units/day\n",
            "* Medium efficiency factories: 1 factory * 60 units/day/factory = 60 units/day (Reconfigured factory)\n",
            "* Low efficiency factories: 1 factory * 30 units/day/factory * 0.5 = 15 units/day (Outaged factory)\n",
            "* **Total production tomorrow: 300 units/day + 60 units/day + 15 units/day = 375 units/day**\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    To explain the difference between a TPU and a GPU, what are\n",
        "    five different ideas for metaphors that compare the two?\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "brainstorm_response = response.text\n",
        "print(brainstorm_response)"
      ],
      "metadata": {
        "id": "EIoG7GQ0_pyK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731325012696,
          "user_tz": -120,
          "elapsed": 5920,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "5d53521e-4670-4ee9-fb8e-f6576f6b5a87"
      },
      "id": "EIoG7GQ0_pyK",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##  Comparing TPUs and GPUs: Five Metaphorical Approaches\n",
            "\n",
            "To effectively explain the distinction between TPUs (Tensor Processing Units) and GPUs (Graphics Processing Units), let's explore five different metaphorical approaches:\n",
            "\n",
            "**1. The Kitchen Analogy:**\n",
            "\n",
            "* **TPU:** Imagine a professional kitchen specifically designed for baking cakes. It has all the specialized equipment and optimized workflow to efficiently produce large quantities of uniform cakes.\n",
            "* **GPU:** This is like a versatile home kitchen equipped to handle various cooking tasks. While it can bake cakes, it can also do many other things like saut√©ing vegetables, grilling meat, or making pasta. \n",
            "\n",
            "**2. The Athlete Analogy:**\n",
            "\n",
            "* **TPU:** A marathon runner, specializing in long-distance running with consistent pace and endurance. \n",
            "* **GPU:** A sprinter, excelling at short bursts of speed and handling diverse tasks.\n",
            "\n",
            "**3. The Manufacturing Analogy:**\n",
            "\n",
            "* **TPU:** A highly automated factory floor, optimized for mass production of a specific product with minimal variation. \n",
            "* **GPU:** A flexible workshop, capable of producing a wider range of products, but with potentially slower output and adjustments needed for different tasks.\n",
            "\n",
            "**4. The Orchestra Analogy:**\n",
            "\n",
            "* **TPU:** A string section playing precisely in unison, excelling at performing the same repetitive task simultaneously.\n",
            "* **GPU:** A full orchestra with various sections and instruments, capable of playing diverse musical pieces with greater complexity.\n",
            "\n",
            "**5. The Transportation Analogy:**\n",
            "\n",
            "* **TPU:** A high-speed train travelling efficiently on a dedicated track, optimized for rapid movement between specific points.\n",
            "* **GPU:** A multi-purpose vehicle, able to navigate different terrains and perform diverse tasks, but potentially with slower speeds on specific routes.\n",
            "\n",
            "Remember, these are just metaphorical representations. Each approach highlights specific strengths and limitations of TPUs and GPUs, helping to understand their core differences in functionality and application. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    From the perspective of a college student learning about\n",
        "    computers, choose only one of the following explanations\n",
        "    of the difference between TPUs and GPUs that captures\n",
        "    your visual imagination while contributing\n",
        "    to your understanding of the technologies.\n",
        "\n",
        "    {brainstorm_response}\n",
        "    \"\"\".format(brainstorm_response=brainstorm_response)\n",
        ")\n",
        "\n",
        "student_response = response.text\n",
        "\n",
        "print(student_response)"
      ],
      "metadata": {
        "id": "LtvvOrZ3_s2N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731325018208,
          "user_tz": -120,
          "elapsed": 5516,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "b6c0b2bc-c2eb-4602-f79a-6527097f9441"
      },
      "id": "LtvvOrZ3_s2N",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## The Kitchen Analogy: Deliciously Imaginative!\n",
            "\n",
            "From the provided options, the kitchen analogy effectively captures my visual imagination and deepens my understanding of TPUs and GPUs. Here's why:\n",
            "\n",
            "**TPU as a Specialized Bakery:** \n",
            "The image of a professional kitchen dedicated to baking cakes resonates strongly. It clearly portrays the TPU's strengths:\n",
            "\n",
            "* **High-performance, specialized hardware:** Like high-end kitchen equipment, TPUs boast optimized hardware specifically designed for AI workloads, especially those involving large datasets and repetitive tasks. \n",
            "* **Efficient workflow:** TPUs streamline the AI process, much like an efficient bakery churns out cakes. This efficiency translates to faster training times and faster results.\n",
            "* **Large-scale output:** The bakery analogy aptly showcases the TPU's ability to process massive amounts of data, just like a bakery churning out countless cakes. This makes TPUs ideal for training large and complex models.\n",
            "\n",
            "**GPU as a Versatile Home Kitchen:**\n",
            "The GPU's portrayal as a versatile home kitchen provides a contrasting perspective:\n",
            "\n",
            "* **Multi-purpose functionality:** Like a resourceful home kitchen, GPUs excel at tackling various tasks, including AI tasks, graphics rendering, and general computing. This versatility makes them suitable for a wider range of applications.\n",
            "* **Adaptability:**  Unlike a factory locked into specific production, a home kitchen adapts to different needs. Similarly, GPUs can handle diverse data types and algorithms, providing flexibility for different AI projects.\n",
            "* **Smaller scale output:** While a home kitchen might not churn out hundreds of cakes at once, it still cooks effectively for smaller batches. This reflects the GPU's ability to handle smaller datasets and less complex models.\n",
            "\n",
            "**Visualizing the Difference:**\n",
            "The kitchen analogy successfully paints a clear picture in my mind. I can imagine the organized efficiency of the bakery churning out cakes, contrasting with the adaptable, multi-tasking environment of a home kitchen. This visualization significantly aids my comprehension of the fundamental differences between TPUs and GPUs.\n",
            "\n",
            "**Overall, the kitchen analogy effectively combines vivid imagery with clear functional distinctions, solidifying my understanding of TPUs and GPUs.** \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    Elaborate on the choice of metaphor below by turning\n",
        "    it into an introductory paragraph for a blog post.\n",
        "\n",
        "    {student_response}\n",
        "    \"\"\".format(student_response=student_response)\n",
        ")\n",
        "\n",
        "blog_post = response.text\n",
        "\n",
        "print(blog_post)"
      ],
      "metadata": {
        "id": "2syLXFR__vZC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731325021870,
          "user_tz": -120,
          "elapsed": 3669,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "a47c0605-04e6-4db1-f575-b1b7453d231f"
      },
      "id": "2syLXFR__vZC",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## The Kitchen Analogy: Cooking Up an Understanding of TPUs and GPUs\n",
            "\n",
            "AI chips, like the human brain, are where the magic happens. But unlike our brains, which are remarkably adaptable, AI chips tend to specialize in different tasks. Today, we'll explore the fascinating world of AI chips through the lens of a familiar setting: the kitchen.\n",
            "\n",
            "Imagine a bustling bakery, churning out hundreds of perfect cakes. This is the TPU, a specialized powerhouse designed for large-scale AI tasks. Its optimized hardware and efficient workflows enable it to process massive datasets and train complex models with lightning speed. Much like a bakery churns out cakes, the TPU delivers results quickly and accurately.\n",
            "\n",
            "Now, picture your home kitchen, a versatile space where you can bake a cake, whip up a stir-fry, or simply reheat your leftovers. This is the GPU, a jack-of-all-trades capable of tackling various AI tasks alongside other computing needs. While it may not specialize in any one area like the TPU, the GPU offers flexibility and adaptability, making it ideal for diverse projects and smaller datasets.\n",
            "\n",
            "So, whether you need a specialized bakery to churn out hundreds of perfect cakes or a versatile home kitchen to handle a variety of smaller tasks, the world of AI offers a chip for every need. And just like the kitchen, the right choice can help you cook up some truly groundbreaking results.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y-4aKkCK_yD8"
      },
      "id": "y-4aKkCK_yD8",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "prompt-design-best-practices.ipynb"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}